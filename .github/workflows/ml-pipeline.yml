name: ML Pipeline with JSON Metrics

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  ml-pipeline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create directories
      run: |
        mkdir -p models data
    
    - name: Run tests
      run: |
        python -m pytest tests/ -v --tb=short
    
    - name: Train model with JSON metrics
      run: |
        echo "ü§ñ Training model with comprehensive JSON metrics..."
        python src/train.py
    
    - name: Verify JSON metrics
      run: |
        echo "üîç Verifying generated files..."
        ls -la models/
        
        echo "üìÑ Checking metrics.json content..."
        if [ -f "models/metrics.json" ]; then
          echo "‚úÖ metrics.json found"
          
          # Display formatted JSON
          python -c "
          import json
          with open('models/metrics.json', 'r') as f:
              metrics = json.load(f)
          print('üìä Model Metrics Summary:')
          print(f'   Model: {metrics.get(\"model_info\", {}).get(\"name\", \"Unknown\")}')
          print(f'   Algorithm: {metrics.get(\"model_info\", {}).get(\"algorithm\", \"Unknown\")}')
          print(f'   Accuracy: {metrics.get(\"performance\", {}).get(\"accuracy\", 0):.4f}')
          print(f'   Classes: {len(metrics.get(\"dataset_info\", {}).get(\"classes\", []))}')
          print(f'   Status: {metrics.get(\"training_info\", {}).get(\"status\", \"Unknown\")}')
          "
        else
          echo "‚ùå metrics.json not found!"
          exit 1
        fi
    
    - name: Test predictions with JSON info
      run: |
        echo "üîÆ Testing predictions..."
        python src/predict.py
    
    - name: Validate model performance from JSON
      run: |
        python -c "
        import json
        import sys
        
        # Load and validate metrics
        with open('models/metrics.json', 'r') as f:
            metrics = json.load(f)
        
        # Extract key information
        status = metrics.get('training_info', {}).get('status', 'unknown')
        accuracy = metrics.get('performance', {}).get('accuracy', 0)
        model_name = metrics.get('model_info', {}).get('name', 'unknown')
        timestamp = metrics.get('training_info', {}).get('timestamp', 'unknown')
        
        print('üéØ Model Validation Results:')
        print('=' * 40)
        print(f'Model Name: {model_name}')
        print(f'Training Status: {status}')
        print(f'Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)')
        print(f'Trained At: {timestamp}')
        
        # Check if training was successful
        if status != 'success':
            print(f'‚ùå Training status indicates failure: {status}')
            if 'error' in metrics.get('training_info', {}):
                print(f'Error: {metrics[\"training_info\"][\"error\"]}')
            sys.exit(1)
        
        # Check accuracy threshold
        min_accuracy = 0.85
        if accuracy < min_accuracy:
            print(f'‚ö†Ô∏è  WARNING: Accuracy {accuracy:.4f} below threshold {min_accuracy}')
            print('This might indicate a training issue or data problem')
            # Don't fail for demo purposes, just warn
        else:
            print(f'‚úÖ Accuracy {accuracy:.4f} meets requirements (>= {min_accuracy})')
        
        # Display additional metrics if available
        if 'dataset_info' in metrics:
            dataset = metrics['dataset_info']
            print(f'Dataset: {dataset.get(\"name\", \"unknown\")} ({dataset.get(\"n_samples\", 0)} samples)')
            print(f'Features: {dataset.get(\"n_features\", 0)}')
            print(f'Classes: {dataset.get(\"n_classes\", 0)}')
        
        if 'performance' in metrics and 'feature_importance' in metrics['performance']:
            print('Top Features:')
            importance = metrics['performance']['feature_importance']
            sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)
            for feature, score in sorted_features[:2]:
                print(f'  {feature}: {score:.4f}')
        
        print('‚úÖ Model validation completed successfully!')
        "
    
    - name: Upload comprehensive artifacts
      uses: actions/upload-artifact@v4
      with:
        name: ml-model-with-metrics-${{ github.sha }}
        path: |
          models/
        retention-days: 30
    
    - name: Success summary
      run: |
        echo "üéâ ML Pipeline with JSON metrics completed!"
        echo "‚úÖ Model trained and validated"
        echo "üìÑ Comprehensive metrics saved to JSON"
        echo "üì¶ Artifacts uploaded with metrics"

  docker-build:
    needs: ml-pipeline
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ml-model-with-metrics-${{ github.sha }}
        path: models/
    
    - name: Verify artifacts with JSON metrics
      run: |
        echo "üì¶ Downloaded artifacts:"
        ls -la models/
        
        if [ -f "models/metrics.json" ]; then
          echo "‚úÖ JSON metrics available for Docker"
          python -c "
          import json
          with open('models/metrics.json', 'r') as f:
              metrics = json.load(f)
          print(f'Docker build using model with {metrics.get(\"performance\", {}).get(\"accuracy\", 0):.4f} accuracy')
          "
        else
          echo "‚ùå JSON metrics missing!"
          exit 1
        fi
    
    - name: Build and test Docker with JSON metrics
      run: |
        echo "üê≥ Building Docker image..."
        docker build -t ml-iris-app:latest .
        
        echo "üß™ Testing Docker with JSON metrics..."
        docker run --rm ml-iris-app:latest python src/predict.py
        
        echo "‚úÖ Docker build and test with JSON metrics successful"