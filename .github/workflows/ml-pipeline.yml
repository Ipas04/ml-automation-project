name: ML Pipeline with JSON Metrics

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  ml-pipeline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create directories
      run: |
        mkdir -p models data
    
    - name: Run tests
      run: |
        python -m pytest tests/ -v --tb=short
    
    - name: Train model with JSON metrics
      run: |
        echo "🤖 Training model with comprehensive JSON metrics..."
        python src/train.py
    
    - name: Verify JSON metrics
      run: |
        echo "🔍 Verifying generated files..."
        ls -la models/
        
        echo "📄 Checking metrics.json content..."
        if [ -f "models/metrics.json" ]; then
          echo "✅ metrics.json found"
          
          # Create separate Python script for metrics display
          cat > display_metrics.py << 'EOF'
        import json
        import sys
        
        try:
            with open('models/metrics.json', 'r') as f:
                metrics = json.load(f)
            
            print('📊 Model Metrics Summary:')
            print('=' * 40)
            
            # Model info
            if 'model_info' in metrics:
                model_info = metrics['model_info']
                print(f'Model: {model_info.get("name", "Unknown")}')
                print(f'Algorithm: {model_info.get("algorithm", "Unknown")}')
            
            # Performance
            if 'performance' in metrics:
                perf = metrics['performance']
                accuracy = perf.get('accuracy', 0)
                print(f'Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)')
            
            # Dataset info
            if 'dataset_info' in metrics:
                dataset = metrics['dataset_info']
                print(f'Dataset: {dataset.get("name", "unknown")}')
                print(f'Classes: {dataset.get("n_classes", 0)}')
                print(f'Features: {dataset.get("n_features", 0)}')
                print(f'Test samples: {dataset.get("test_size", 0)}')
            
            # Training info
            if 'training_info' in metrics:
                training = metrics['training_info']
                print(f'Status: {training.get("status", "unknown")}')
                print(f'Timestamp: {training.get("timestamp", "unknown")}')
            
            print('✅ Metrics loaded successfully!')
            
        except Exception as e:
            print(f'❌ Error loading metrics: {e}')
            sys.exit(1)
        EOF
          
          # Run the metrics display script
          python display_metrics.py
          
        else
          echo "❌ metrics.json not found!"
          exit 1
        fi
    
    - name: Test predictions with JSON info
      run: |
        echo "🔮 Testing predictions..."
        python src/predict.py
    
    - name: Validate model performance from JSON
      run: |
        # Create validation script
        cat > validate_model.py << 'EOF'
        import json
        import sys
        
        try:
            # Load metrics
            with open('models/metrics.json', 'r') as f:
                metrics = json.load(f)
            
            # Extract key information
            status = metrics.get('training_info', {}).get('status', 'unknown')
            accuracy = metrics.get('performance', {}).get('accuracy', 0)
            model_name = metrics.get('model_info', {}).get('name', 'unknown')
            
            print('🎯 Model Validation Results:')
            print('=' * 40)
            print(f'Model Name: {model_name}')
            print(f'Training Status: {status}')
            print(f'Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)')
            
            # Check training status
            if status != 'success':
                print(f'❌ Training status indicates failure: {status}')
                if 'error' in metrics.get('training_info', {}):
                    print(f'Error: {metrics["training_info"]["error"]}')
                sys.exit(1)
            
            # Check accuracy threshold
            min_accuracy = 0.85
            if accuracy < min_accuracy:
                print(f'⚠️  WARNING: Accuracy {accuracy:.4f} below threshold {min_accuracy}')
                print('Model performance may need improvement')
                # Don't fail for demo, just warn
            else:
                print(f'✅ Accuracy {accuracy:.4f} meets requirements (>= {min_accuracy})')
            
            # Display feature importance if available
            if 'performance' in metrics and 'feature_importance' in metrics['performance']:
                print('\n📈 Feature Importance:')
                importance = metrics['performance']['feature_importance']
                sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)
                for i, (feature, score) in enumerate(sorted_features, 1):
                    print(f'  {i}. {feature}: {score:.4f}')
            
            print('\n✅ Model validation completed successfully!')
            
        except FileNotFoundError:
            print('❌ metrics.json file not found!')
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f'❌ Error parsing JSON: {e}')
            sys.exit(1)
        except Exception as e:
            print(f'❌ Validation error: {e}')
            sys.exit(1)
        EOF
        
        # Run validation
        python validate_model.py
    
    - name: Upload comprehensive artifacts
      uses: actions/upload-artifact@v4
      with:
        name: ml-model-with-metrics-${{ github.sha }}
        path: |
          models/
        retention-days: 30
    
    - name: Success summary
      run: |
        echo "🎉 ML Pipeline with JSON metrics completed!"
        echo "✅ Model trained and validated"
        echo "📄 Comprehensive metrics saved to JSON"
        echo "📦 Artifacts uploaded with metrics"

  docker-build:
    needs: ml-pipeline
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ml-model-with-metrics-${{ github.sha }}
        path: models/
    
    - name: Verify artifacts with JSON metrics
      run: |
        echo "📦 Downloaded artifacts:"
        ls -la models/
        
        if [ -f "models/metrics.json" ]; then
          echo "✅ JSON metrics available for Docker"
          
          # Quick metrics check
          python -c "
        import json
        with open('models/metrics.json', 'r') as f:
            metrics = json.load(f)
        accuracy = metrics.get('performance', {}).get('accuracy', 0)
        print(f'Docker build using model with {accuracy:.4f} accuracy')
          "
        else
          echo "❌ JSON metrics missing!"
          exit 1
        fi
    
    - name: Build and test Docker with JSON metrics
      run: |
        echo "🐳 Building Docker image..."
        docker build -t ml-iris-app:latest .
        
        echo "🧪 Testing Docker with JSON metrics..."
        docker run --rm ml-iris-app:latest python src/predict.py
        
        echo "✅ Docker build and test with JSON metrics successful"